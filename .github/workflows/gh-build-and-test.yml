on: workflow_call

jobs:
  build:
    strategy:
      fail-fast: false
      matrix:
        # TODO: align host-platform names with conda convention
        host-platform:
          - linux-x64
          - linux-aarch64
          - win-x64
        python-version:
          - "3.13"
          - "3.12"
          - "3.11"
          - "3.10"
          - "3.9"
        cuda-version:
          # Note: this is for build-time only.
          - "12.6.2"
    name: Build (${{ matrix.host-platform }}, Python "${{ matrix.python-version }}")
    if: ${{ github.repository_owner == 'nvidia' }}
    permissions:
      id-token: write # This is required for configure-aws-credentials
      contents: read  # This is required for actions/checkout
    runs-on: ${{ (matrix.host-platform == 'linux-x64' && 'linux-amd64-cpu8') ||
                 (matrix.host-platform == 'linux-aarch64' && 'linux-arm64-cpu8') ||
                 (matrix.host-platform == 'win-x64' && 'windows-2019') }}
               #  (matrix.host-platform == 'win-x64' && 'windows-amd64-cpu8') }}
    outputs:
      CUDA_CORE_ARTIFACT_NAME: ${{ steps.pass_env.outputs.CUDA_CORE_ARTIFACT_NAME }}
      CUDA_CORE_ARTIFACTS_DIR: ${{ steps.pass_env.outputs.CUDA_CORE_ARTIFACTS_DIR }}
      CUDA_BINDINGS_ARTIFACT_NAME: ${{ steps.pass_env.outputs.CUDA_BINDINGS_ARTIFACT_NAME }}
      CUDA_BINDINGS_ARTIFACTS_DIR: ${{ steps.pass_env.outputs.CUDA_BINDINGS_ARTIFACTS_DIR }}
    steps:
      - name: Checkout ${{ github.event.repository.name }}
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      # WAR: setup-python is not relocatable...
      # see https://github.com/actions/setup-python/issues/871
      - name: Set up Python ${{ matrix.python-version }}
        if: ${{ startsWith(matrix.host-platform, 'linux') }}
        id: setup-python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"
  
      - name: Set up MSVC
        if: ${{ startsWith(matrix.host-platform, 'win') }}
        uses: ilammy/msvc-dev-cmd@v1
  
      - name: Set environment variables
        shell: bash --noprofile --norc -xeuo pipefail {0}
        run: |
          PYTHON_VERSION_FORMATTED=$(echo '${{ matrix.python-version }}' | tr -d '.')
          if [[ "${{ matrix.host-platform }}" == linux* ]]; then
            CIBW_BUILD="cp${PYTHON_VERSION_FORMATTED}-manylinux*"
            REPO_DIR=$(pwd)
          elif [[ "${{ matrix.host-platform }}" == win* ]]; then
            CIBW_BUILD="cp${PYTHON_VERSION_FORMATTED}-win_amd64"
            PWD=$(pwd)
            REPO_DIR=$(cygpath -w $PWD)
          fi
  
          echo "PARALLEL_LEVEL=$(nproc)" >> $GITHUB_ENV
          echo "CUDA_CORE_ARTIFACT_NAME=cuda-core-python${PYTHON_VERSION_FORMATTED}-${{ matrix.host-platform }}-${{ github.sha }}" >> $GITHUB_ENV
          echo "CUDA_CORE_ARTIFACTS_DIR=$(realpath "$REPO_DIR/cuda_core/dist")" >> $GITHUB_ENV
          echo "CUDA_BINDINGS_ARTIFACT_NAME=cuda-bindings-python${PYTHON_VERSION_FORMATTED}-cuda${{ matrix.cuda-version }}-${{ matrix.host-platform }}-${{ github.sha }}" >> $GITHUB_ENV
          echo "CUDA_BINDINGS_ARTIFACTS_DIR=$(realpath "$REPO_DIR/cuda_bindings/dist")" >> $GITHUB_ENV
          echo "CIBW_BUILD=${CIBW_BUILD}" >> $GITHUB_ENV
  
      - name: Dump environment
        shell: bash --noprofile --norc -xeuo pipefail {0}
        run: |
          env

      - name: Build cuda.core wheel
        uses: pypa/cibuildwheel@v2.22.0
        env:
          CIBW_BUILD: ${{ env.CIBW_BUILD }}
          CIBW_ARCHS_LINUX: "native"
          CIBW_BUILD_VERBOSITY: 1
        with:
          package-dir: ./cuda_core/
          output-dir: ${{ env.CUDA_CORE_ARTIFACTS_DIR }}

      - name: List the cuda.core artifacts directory
        shell: bash --noprofile --norc -xeuo pipefail {0}
        run: |
          if [[ "${{ matrix.host-platform }}" == win* ]]; then
            export CHOWN=chown
          else
            export CHOWN="sudo chown"
          fi
          $CHOWN -R $(whoami) ${{ env.CUDA_CORE_ARTIFACTS_DIR }}
          ls -lahR ${{ env.CUDA_CORE_ARTIFACTS_DIR }}

      - name: Check cuda.core wheel
        shell: bash --noprofile --norc -xeuo pipefail {0}
        run: |
          pip install twine
          twine check ${{ env.CUDA_CORE_ARTIFACTS_DIR }}/*.whl

      - name: Upload cuda.core build artifacts
        uses: actions/upload-artifact@v4
        with:
          name: ${{ env.CUDA_CORE_ARTIFACT_NAME }}
          path: ${{ env.CUDA_CORE_ARTIFACTS_DIR }}/*.whl
          if-no-files-found: error
          overwrite: 'true'

      - name: Set up mini CTK
        uses: ./.github/actions/fetch_ctk
        continue-on-error: false
        with:
          host-platform: ${{ matrix.host-platform }}
          cuda-version: ${{ matrix.cuda-version }}
          fail-on-ctk-cache-miss: false

      - name: Build cuda.bindings wheel
        uses: pypa/cibuildwheel@v2.22.0
        env:
          CIBW_BUILD: ${{ env.CIBW_BUILD }}
          CIBW_ARCHS_LINUX: "native"
          CIBW_BUILD_VERBOSITY: 1
          # CIBW mounts the host filesystem under /host
          CIBW_ENVIRONMENT_LINUX: >
            CUDA_PATH=/host/${{ env.CUDA_PATH }}
            PARALLEL_LEVEL=${{ env.PARALLEL_LEVEL }}
          CIBW_ENVIRONMENT_WINDOWS: >
            CUDA_HOME="$(cygpath -w ${{ env.CUDA_PATH }})"
          #  PARALLEL_LEVEL=${{ env.PARALLEL_LEVEL }}
        with:
          package-dir: ./cuda_bindings/
          output-dir: ${{ env.CUDA_BINDINGS_ARTIFACTS_DIR }}

      - name: List the cuda.bindings artifacts directory
        shell: bash --noprofile --norc -xeuo pipefail {0}
        run: |
          if [[ "${{ matrix.host-platform }}" == win* ]]; then
            export CHOWN=chown
          else
            export CHOWN="sudo chown"
          fi
          $CHOWN -R $(whoami) ${{ env.CUDA_BINDINGS_ARTIFACTS_DIR }}
          ls -lahR ${{ env.CUDA_BINDINGS_ARTIFACTS_DIR }}

      # TODO: enable this after NVIDIA/cuda-python#297 is resolved
      # - name: Check cuda.bindings wheel
      #   shell: bash --noprofile --norc -xeuo pipefail {0}
      #   run: |
      #     twine check ${{ env.CUDA_BINDINGS_ARTIFACTS_DIR }}/*.whl

      - name: Upload cuda.bindings build artifacts
        uses: actions/upload-artifact@v4
        with:
          name: ${{ env.CUDA_BINDINGS_ARTIFACT_NAME }}
          path: ${{ env.CUDA_BINDINGS_ARTIFACTS_DIR }}/*.whl
          if-no-files-found: error
          overwrite: 'true'

      - name: Pass environment variables to the next runner
        id: pass_env
        run: |
          echo "CUDA_CORE_ARTIFACT_NAME=${CUDA_CORE_ARTIFACT_NAME}" >> $GITHUB_OUTPUT
          echo "CUDA_CORE_ARTIFACTS_DIR=${CUDA_CORE_ARTIFACTS_DIR}" >> $GITHUB_OUTPUT
          echo "CUDA_BINDINGS_ARTIFACT_NAME=${CUDA_BINDINGS_ARTIFACT_NAME}" >> $GITHUB_OUTPUT
          echo "CUDA_BINDINGS_ARTIFACTS_DIR=${CUDA_BINDINGS_ARTIFACTS_DIR}" >> $GITHUB_OUTPUT

  test:
    strategy:
      fail-fast: false
      matrix:
        # TODO: align host-platform names with conda convention
        host-platform:
          - linux-x64
          - linux-aarch64
          # TODO: enable testing once win-64 GPU runners are up
          # - win-x64
        python-version:
          - "3.13"
          - "3.12"
          - "3.11"
          - "3.10"
          - "3.9"
        cuda-version:
          # Note: this is for test-time only.
          - "12.6.2"
          - "12.0.1"
          - "11.8.0"
    name: Test (${{ matrix.host-platform }}, CUDA ${{ matrix.cuda-version }}, Python "${{ matrix.python-version }}")
    if: ${{ (github.repository_owner == 'nvidia') }}
    permissions:
      id-token: write # This is required for configure-aws-credentials
      contents: read  # This is required for actions/checkout
    runs-on: ${{ (matrix.host-platform == 'linux-x64' && 'linux-amd64-gpu-v100-latest-1') ||
                 (matrix.host-platform == 'linux-aarch64' && 'linux-arm64-gpu-a100-latest-1') }}
    # Our self-hosted runners require a container
    # TODO: use a different (nvidia?) container
    container:
      options: -u root --security-opt seccomp=unconfined --shm-size 16g
      image: ubuntu:22.04
      env:
        NVIDIA_VISIBLE_DEVICES: ${{ env.NVIDIA_VISIBLE_DEVICES }}
    needs:
      - build
    steps:
      - name: Run nvidia-smi to make sure GPU is working
        shell: bash --noprofile --norc -xeuo pipefail {0}
        run: nvidia-smi

      - name: Checkout ${{ github.event.repository.name }}
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up test environment
        shell: bash --noprofile --norc -xeuo pipefail {0}
        run: |
          # make outputs from the previous job as env vars
          echo "CUDA_CORE_ARTIFACT_NAME=${{ needs.build.outputs.CUDA_CORE_ARTIFACT_NAME }}" >> $GITHUB_ENV
          echo "CUDA_CORE_ARTIFACTS_DIR=${{ needs.build.outputs.CUDA_CORE_ARTIFACTS_DIR }}" >> $GITHUB_ENV
          echo "CUDA_BINDINGS_ARTIFACT_NAME=${{ needs.build.outputs.CUDA_BINDINGS_ARTIFACT_NAME }}" >> $GITHUB_ENV
          echo "CUDA_BINDINGS_ARTIFACTS_DIR=${{ needs.build.outputs.CUDA_BINDINGS_ARTIFACTS_DIR }}" >> $GITHUB_ENV

      - name: Download bindings build artifacts
        uses: actions/download-artifact@v4
        with:
          name: ${{ env.CUDA_BINDINGS_ARTIFACT_NAME }}
          path: ${{ env.CUDA_BINDINGS_ARTIFACTS_DIR }}

      - name: Display structure of downloaded bindings artifacts
        shell: bash --noprofile --norc -xeuo pipefail {0}
        run: |
          pwd
          ls -lahR $CUDA_BINDINGS_ARTIFACTS_DIR

      - name: Download core build artifacts
        uses: actions/download-artifact@v4
        with:
          name: ${{ env.CUDA_CORE_ARTIFACT_NAME }}
          path: ${{ env.CUDA_CORE_ARTIFACTS_DIR }}

      - name: Display structure of downloaded core build artifacts
        shell: bash --noprofile --norc -xeuo pipefail {0}
        run: |
          pwd
          ls -lahR $CUDA_CORE_ARTIFACTS_DIR

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}

      # The cache action needs this
      - name: Install zstd
        shell: bash --noprofile --norc -xeuo pipefail {0}
        run: |
          apt update
          apt install zstd

      - name: Set up mini CTK
        uses: ./.github/actions/fetch_ctk
        continue-on-error: false
        with:
          host-platform: ${{ matrix.host-platform }}
          cuda-version: ${{ matrix.cuda-version }}
          fail-on-ctk-cache-miss: true

      - name: Run test / analysis
        shell: bash --noprofile --norc -xeuo pipefail {0}
        run: |
          ls $CUDA_PATH

          REPO_DIR=$(pwd)

          cd "${CUDA_BINDINGS_ARTIFACTS_DIR}"
          pip install *.whl

          cd "${CUDA_CORE_ARTIFACTS_DIR}"
          pip install *.whl

          cd "${REPO_DIR}/cuda_bindings"
          pip install -r requirements.txt
          pytest -rxXs tests/
          # TODO: enable cython tests
          #pytest tests/cython

          cd "${REPO_DIR}/cuda_core"
          pytest -rxXs tests/
